{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4c28884-595a-4752-b1b2-fbfd0512310a",
   "metadata": {},
   "source": [
    "# Load in the data - X-Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "209efd00-e178-43ad-b0cc-0c6208f8a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.chdir(r'C:\\Users\\annal\\OneDrive\\Desktop\\llm-misinfo-eval\\data')  #this line is what I've been using to solve a no such directory error - copy paste the path of data folder\n",
    "df = pd.read_csv(r\"x_fact_dataset/x-fact-including-en/zeroshot.tsv\", delimiter=\"\\t\", on_bad_lines=\"skip\")\n",
    "#TODO read in right csv!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877158c7",
   "metadata": {},
   "source": [
    "# Get results from Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dbb4116-9254-4c35-8ebe-407a86830f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prompt_1(STATEMENT):\n",
    "    return f\"Rate the truthfulness of the following statement: “{STATEMENT}” Provide a score from 0 to 100, where 0 represents definitively false and 100 represents definitively true. Do not provide any explanations, only respond with the numerical score.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04041658-e694-407e-b3d3-370c26643c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "from vertexai.preview.generative_models import GenerativeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f9d05359-091f-489d-8b8b-009a1c52bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(prompt):\n",
    "    vertexai.init(project='llm-misinfo-eval', location='us-central1')\n",
    "    parameters = {\n",
    "        # 'temperature':0,\n",
    "        # 'max_output_tokens':256,\n",
    "        # 'top_p':0.8,\n",
    "        # 'top_k':40\n",
    "    }\n",
    "    model = GenerativeModel('gemini-1.0-pro')\n",
    "    resp = model.generate_content(prompt, **parameters)\n",
    "    return resp\n",
    "\n",
    "def get_text_resp(model_resp):\n",
    "    return model_resp.candidates[0].content.parts[0].text\n",
    "# resp = get_text_resp(call_model('what is your name?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7eb49dd7-5d8b-48ff-afbc-81cfd3243718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██████████████████▉                                                            | 72/300 [21:31<1:08:11, 17.94s/it]\n"
     ]
    },
    {
     "ename": "InternalServerError",
     "evalue": "500 Internal error encountered.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\grpc\\_channel.py:1176\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1170\u001b[0m (\n\u001b[0;32m   1171\u001b[0m     state,\n\u001b[0;32m   1172\u001b[0m     call,\n\u001b[0;32m   1173\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[0;32m   1174\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[0;32m   1175\u001b[0m )\n\u001b[1;32m-> 1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\grpc\\_channel.py:1005\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[1;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1005\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[1;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"Internal error encountered.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:172.217.13.202:443 {grpc_message:\"Internal error encountered.\", grpc_status:13, created_time:\"2024-02-28T22:32:51.0385889+00:00\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m claim \u001b[38;5;241m=\u001b[39m df_x_fact[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclaim\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\n\u001b[0;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m return_prompt_1(claim)\n\u001b[1;32m----> 8\u001b[0m model_resp \u001b[38;5;241m=\u001b[39m \u001b[43mcall_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m model_text_resp \u001b[38;5;241m=\u001b[39m get_text_resp(model_resp)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[68], line 10\u001b[0m, in \u001b[0;36mcall_model\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m      3\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# 'temperature':0,\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# 'max_output_tokens':256,\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# 'top_p':0.8,\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# 'top_k':40\u001b[39;00m\n\u001b[0;32m      8\u001b[0m }\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m GenerativeModel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini-1.0-pro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\vertexai\\generative_models\\_generative_models.py:354\u001b[0m, in \u001b[0;36m_GenerativeModel.generate_content\u001b[1;34m(self, contents, generation_config, safety_settings, tools, stream)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_content_streaming(\n\u001b[0;32m    348\u001b[0m         contents\u001b[38;5;241m=\u001b[39mcontents,\n\u001b[0;32m    349\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m    350\u001b[0m         safety_settings\u001b[38;5;241m=\u001b[39msafety_settings,\n\u001b[0;32m    351\u001b[0m         tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m    352\u001b[0m     )\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafety_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\vertexai\\generative_models\\_generative_models.py:435\u001b[0m, in \u001b[0;36m_GenerativeModel._generate_content\u001b[1;34m(self, contents, generation_config, safety_settings, tools)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generates content.\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \n\u001b[0;32m    414\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;124;03m    A single GenerationResponse object\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    429\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[0;32m    430\u001b[0m     contents\u001b[38;5;241m=\u001b[39mcontents,\n\u001b[0;32m    431\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m    432\u001b[0m     safety_settings\u001b[38;5;241m=\u001b[39msafety_settings,\n\u001b[0;32m    433\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m    434\u001b[0m )\n\u001b[1;32m--> 435\u001b[0m gapic_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prediction_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_response(gapic_response)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\cloud\\aiplatform_v1beta1\\services\\prediction_service\\client.py:2075\u001b[0m, in \u001b[0;36mPredictionServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m   2072\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m-> 2075\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2082\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m   2083\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mInternalServerError\u001b[0m: 500 Internal error encountered."
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "import time\n",
    "df_results_prompt_1_list = []\n",
    "for i in trange(300):\n",
    "    tmp = {'modelResp':None, 'trueLabel':df['label'][i]}\n",
    "    claim = df['claim'][i]\n",
    "    prompt = return_prompt_1(claim)\n",
    "    model_resp = call_model(prompt)\n",
    "    model_text_resp = get_text_resp(model_resp)\n",
    "    try:\n",
    "        tmp['modelResp'] = float(model_text_resp)\n",
    "    except:\n",
    "        tmp['modelResp'] = float(model_text_resp)\n",
    "    df_results_prompt_1_list.append(tmp)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24da006",
   "metadata": {},
   "source": [
    "# Saving Gemini's Responses to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "49447e14-3b6a-43da-ac06-9d3f6cd7faaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 2)\n"
     ]
    }
   ],
   "source": [
    "df_results_prompt_1 = pd.DataFrame(df_results_prompt_1_list)\n",
    "df_results_prompt_1.to_csv(\"gemini_results\", sep=',', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
